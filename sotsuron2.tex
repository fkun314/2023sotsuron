\documentclass[a4j,12pt,dvipdfmx]{jreport}
\usepackage[top=35mm, bottom=30mm, left=30mm, right=30mm]{geometry}

% \oddsidemargin= 5mm   
% \textwidth = 160mm
% \textheight = 205mm
% \addtolength{\textheight}{3cm}
% %\addtolength{\textwidth}{1.5cm}
% \topmargin = -5mm
% \headheight = 0mm
% \footskip = 10mm

% % 図形に関する処理
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\dbltopfraction}{1.0}
\renewcommand{\textfraction}{0.01}
\renewcommand{\floatpagefraction}{1.0}
\renewcommand{\dblfloatpagefraction}{1.0}
\setcounter{topnumber}{5}
\setcounter{bottomnumber}{5}
\setcounter{totalnumber}{10}

\usepackage{url}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath,amsthm,amssymb,ascmac}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{fancyvrb}
\usepackage[T1]{fontenc}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{here}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}

\renewcommand{\bibname}{参考文献}

\begin{document}
\pagenumbering{roman}
%表紙
\begin{titlepage} \centering
  %\LARGE
  \null \null
  \vspace{10mm}
  令和04年度　卒業研究論文\\
  %\vfill
  \vspace{55.15mm}
  \null
  % 研究題目\\
  {\Large CNNを用いた動画検索用インデックス作成に関する研究}\\
  %\vfill \vfill
  %\vfill
  
  \vspace{87.5mm}
  %{\Large 平成24年2月23日}
  %\vspace{15mm}
  
  \large
  総合工学科 情報システムコース 5年\\[1mm]
  1801113 藤田 晴斗\\[1mm]
  指導教員 総合工学科 藤原 和彦\\
  
  \vspace{16mm}
  {仙台高等専門学校}
  \null

\end{titlepage}

%目次
% \tableofcontents
% \clearpage
\clearpage \input{content.tex}{\clearpage}
\large
\renewcommand{\baselinestretch}{1.1}

\chapter{序論}
\pagenumbering{arabic}
\label{sec:introducion}

\section{研究背景}
近年，スマートフォンや動画ストリーミングサービスの急速な普及に伴い，個人が取り扱う動画の本数が大幅に増加している．
2020年にGoogle社が発表したYouTube（動画ストリーミングサービス）にアップロードされた動画の総時間について，2019年と比較して80％も増加していることがわかった\cite{google_data}．
そこで，個人が取り扱う動画について，より効率的なデータ管理を行うためのシステムが求められていると考えられる．

これまで，動画内で入力した画像と一致するシーンを検索するには，動画の各フレームを画像のように扱い，フレーム同士の各座標の画素値（RGB値）が完全に一致しているものを探す必要があった．
また，動画内から画像の類似シーンを検索しようとしたときには，互いの画素が一致している割合を用いて比較・検討が行われる．
そこで実際の画像（図\ref{fig:hikaku}）を用いた類似度の推定を行なってみた．

画像のヒストグラムから類似度を計算するプログラムでは，この2枚の画像は類似度36％であると推定された．
しかし，構成している物体が同じであるため，類似した画像として扱えるのではないかと考えた．
この仮説のもと，広い意味での類似したシーン検索ができるよう，これまでのRGB値での比較を行うのではなく，画像を構成している物体に着目した比較を行うことができるようなシステム構築を目指した．

\vspace{2zh}
\begin{figure}[H]
  \centering
  \fbox{\includegraphics[width=14cm]{image/hikaku.png}}
  \caption{\label{fig:hikaku} 比較サンプル画像}
\end{figure}

\section{研究目的}
従来，個人がデータとして所持している動画データの管理方法は，動画のタイトルや長さ，重さ，撮影時間などであった．
ここに，サードパーティー製のソフトウェアを使用することによって，クラウド上で動画内に何が映っているかの分析が可能となり，より動画の管理が容易になった．

しかし，画像と動画では同じ分析結果を使用できるわけではない．
画像では何が写っているかが分かるとおおよそ画像の整理や検索を行うことができるが，動画では何が映っているかの情報に，時系列情報が付加されている．

本研究では，従来のシステムでは動画に対して時系列情報が付加されていないことに注目し，動画内の物体検出並びに時系列情報を用いた解析を可能とすることを目的とする．

さらに，画像を入力元として扱える動画検索機能を作成することも目的とする．
この機能を実装することにより，これまで動画のワンシーンを「動画のタイトル・ファイル名」と「時間・再生位置」などで管理していたところを，該当するシーンの「スクリーンショット」や「類似している別の画像」を使用することで，対象となる動画および時間を推定することが可能となる．
これによって人力では非常に困難とされている大量の動画データからの検索がより身近なものになると考えられる．

\section{本論文の構成}
以下に本論文の構成を示す．
\begin{description}
\item[第\ref{sec:introducion}章 \qquad 序論]\mbox{}\\
本システムの概要を示す．

\item[第\ref{sec:algorithm}章 \qquad 画像処理アルゴリズムについて]\mbox{}\\
本システムを開発するにあたって参考にした画像処理アルゴリズムについて述べる．

\item[第\ref{sec:system}章 \qquad 動画解析システムについて]\mbox{}\\
本システムの全体の仕組みや，使用した技術について述べる．

\item[第\ref{sec:consideration}章 \qquad 動作検証と考察]\mbox{}\\
動作結果とともに，本システムの課題や今後の展望について述べる．

\item[第\ref{sec:conclusion}章 \qquad 結論]\mbox{}\\
本システムの開発にあたっての目的と，その達成度について述べる．

\end{description}

\clearpage

\chapter{画像処理アルゴリズムについて}
\label{sec:algorithm}

\section{はじめに}
本章では，本研究を行うにあたり必要な画像処理アルゴリズムと，その基本となる学習処理について述べる.

\section{ニューラルネットワーク}
ニューラルネットワーク (Artificial Neural Network; ANN) とは，人工知能における学習アルゴリズムの一種のことである\cite{ANN}．
ニューラルネットワークは，人工ニューロンと呼ばれるユニットを組み合わせて構成される．

\subsection{人工ニューロン}
人工ニューロンとは，生物学的な神経細胞を模倣して構築された数学的なモデルである\cite{ms_tamura}．

人工ニューロンの構成を図\ref{fig:neuron}に示す．

\begin{figure}[b]
  \centering
  \fbox{
    \includegraphics[width=7cm]{image/neuron.png}
  }
  \caption{人工ニューロン}
  \label{fig:neuron}
\end{figure}

人工ニューロンは，外部から与えられたデータやネットワークを構成する他の人工ニューロンから複数の入力を受け取り，適当な計算を施して出力信号を出力する．
図\ref{fig:neuron}より，入力された値$x_i$は，入力ごとに重みと呼ばれる定数$w_i$を掛け合わせる．
入力信号は重みを掛け合わせた上で足し合わせ，しきい値と呼ばれる定数$v$を減算する．
こうして計算を行なった積算値$u$を，伝達関数(transfer function)で処理した結果$f(u)$を人工ニューロンの出力$y$とする．
以上の過程を次の(\ref{eq:sum})式で表現する．

\begin{equation}
  \label{eq:sum}
  \begin{split}
    u &= \sum_{i=1}^n w_ix_i - v \\
    y &= f(u)
  \end{split}
\end{equation}

伝達関数にはステップ関数(step function)やシグモイド関数(sigmoid function)などがよく用いられる\cite{ANN}\cite{G}．
ステップ関数は，入力が0以上であれば1を返し，0未満であれば0を返す非線形関数である．
シグモイド関数とは，次の(\ref{eq:frac1})式のような関数である．

\begin{eqnarray}
  \label{eq:frac1}
  f(u) = \frac{1}{1 + \mathrm{e}^{-u}}
\end{eqnarray}

入力$u$が増加すると，出力$f(u)$は1に近づき，入力$x$が減少すると，出力$f(u)$は0に近づく．
つまり，シグモイド関数は入力$u$がどの程度大きいかに応じて，出力$f(u)$がどの程度大きくなるかを制御することが可能となっている．

ただし．シグモイド関数はANNにおいては，勾配消失問題が発生しやすいため，シグモイド関数に代わる別の関数が用いられることが多い．
勾配消失問題については，後述する．

\subsection{ニューラルネットワーク}
ニューラルネットワークの構成図を図\ref{fig:perceptron}に示す．

\begin{figure}[t]
  \centering
  \fbox{\includegraphics[width=12cm]{image/perceptron.png}}
  \caption{ニューラルネットワークの構成図}
  \label{fig:perceptron}
\end{figure}
ニューラルネットワークは人工ニューロンをいくつも組み合わせて構成される\cite{ms_tamura}．
多層パーセプトロンと呼ばれる構造（図\ref{fig:perceptron}）は，入力層，中間層，出力層からなる．
なお，単純パーセプトロンと呼ばれる構造もあり，こちらは中間層がない．

入力層は入力信号を次段の人工ニューロンに伝えるだけの固定化した素子である．
入力層から中間層に向かう重みとしきい値は，ランダムに初期化した固定の数値である．
出力層の重みとしきい値は変更可能である．

パーセプトロンは，各階層はいずれも似たような構造であり，階層間の結合も全結合である．


\subsection{学習について}
学習とは，ANN中の重みとしきい値を調整することで，基本的に教師あり学習と呼ばれる手法を用いる\cite{G}．
教師あり学習とは，教師データと呼ばれる正解データを用いて学習を行う手法である．
教師データには，入力データと正解データが含まれている．
教師データを用いて学習を行うことで，パーセプトロンは入力データに対して正解データを出力するように学習する．

つまり，学習とは，入力された値が正解データと一致するように，パーセプトロンの重みとしきい値を調整することである．

\subsection{誤差逆伝播法}
誤差逆伝播法とは，ニューラルネットワークの学習アルゴリズムの一種で，誤差を伝播させて，各層の重みとしきい値を更新することで，ニューラルネットワークを学習させる手法である\cite{ANN}\cite{G}．
損失関数 (loss function) と呼ばれる，ニューラルネットワークの予測結果と正解との誤差を最小化することで，より精度の高い結果を得ることができる．

一般的に，逆誤差伝播法は勾配降下法 (Gradient Descent) と組み合わせて使用される．
勾配降下法とは，損失関数を最小化するために重みとバイアスを更新するためのアルゴリズムである．

出力から入力に逆伝播していく際に，伝播関数の部分を微分する必要がある．
活性化関数にシグモイド関数を用いた場合，逆伝播を繰り返すと，微分の結果が0に近づいてしまうという問題がある．
このことを勾配消失問題と呼ぶ．

シグモイド関数と，それを微分した関数を図\ref{fig:sigmoid}に示す．
この例では，微分後の関数の最大値が0.25となるため，隠れ層を繰り返すごとに伝播していく誤差が小さくなってしまう．
そうなると，入力層まで遡った時のフィードバックする値が0に近い値となり，学習が行えなくなってしまう．

\begin{figure}[b]
  \centering
  \includegraphics[width=10cm]{image/sigmoid.jpg}
  \caption{シグモイド関数と微分した関数}
  \label{fig:sigmoid}
\end{figure}

この問題を解決するために，活性化関数にはReLU関数やLeaky ReLU関数などが用いられるが多い．

ReLU関数（Rectified Linear Unit）は，入力が0より大きければそのまま出力し，0以下であれば0を出力する関数である．
ただし，入力値が0以下の場合，勾配が0になってしまうため，微分した関数の最大値が0となってしまう．
この問題を解決するために，Leaky ReLU関数が提案された．
Leaky ReLU関数は，入力が0以下の場合，微小な値を出力する関数である．

ReLU関数とLeaky ReLU関数を図\ref{fig:relu}に示す．
これらの活性化関数を用いることにより，勾配消失問題を解決することができ，より深い階層のニューラルネットワークを構築することが可能となる．

\begin{figure}[b]
  \centering
  \includegraphics[width=10cm]{image/relu.png}
  \caption{ReLU関数とLeaky ReLU関数}
  \label{fig:relu}
\end{figure}

\section{ディープラーニング(深層学習)}
ディープラーニングとは，人間が行う作業をコンピュータに学習させる機械学習の一種である\cite{G}．
人間の神経をモデルとしたニューラルネットワークを多層化し，コンピュータ自身が大量のデータから特徴を抽出し，予測や分類を行う手法のことである．
ディープラーニングは，ディープニューラルネットワークとも呼ばれる．

\section{CNN(Convolutional Neural Network)}
CNNとは，畳み込みニューラルネットワークとも呼ばれ，画像などの処理に特化した多階層の階層型ニューラルネットワークである\cite{cnn}． 
CNNの基本構造を図\ref{fig:cnn}に示す．

\begin{figure}[b]
  \centering
  \fbox{\includegraphics[width=13cm]{image/cnn.jpeg}}
  \caption{CNNの基本構造}
  \label{fig:cnn}
\end{figure}

CNNにはパーセプトロンの構造とは異なり，畳み込み層(convolutional layer)と呼ばれる層とプーリング層(pooling layer)と呼ばれる層が存在する．

畳み込み層 (Convolutional Layer)は，カーネルと呼ばれる畳み込みフィルタ (Convolutional Filter) を用いて画像から特徴を抽出する操作のことである．
畳み込みフィルタを複数使用し，畳み込み演算を通して画像の特徴がより強く現れる特徴マップ(Feature Map)を作成する．
この処理では，人間の視覚野が持つ局所受容野に対応しており，移動普遍性の獲得に貢献する．
つまり，畳み込みの処理を通して「位置のずれ」に強いモデルの作成が可能となる．

プーリング層(Pooling Layer)の役割は，畳み込み層で作られた特徴マップをより小さなものへと変換し，情報の位置ズレに対する頑強さを強めることにある．
プーリング操作として，maxプーリングと呼ばれる特徴マップの最大値を抽出するものや，aveプーリングと呼ばれる特徴マップの平均値を抽出するものがある．

最後に，全結合層(Fully Connected Layer)で情報を縮約することで，ニューラルネットワークの処理が容易になる．

\section{まとめ}
本システムで取り扱っている画像処理アルゴリズムと，CNNを中心としたディープラーニングについて説明した．
\clearpage
\chapter{動画解析システムについて}
\label{sec:system}

\section{はじめに}
\label{chap3-1}
本章では，本システムの動画解析システムについて説明する．

\section{使用するライブラリについて}
\label{chap3-2}
本研究ではFacebook AI Research(FAIR)が開発したDetectron2を使用する．
Detectron2とは，高機能な物体検出およびセグメンテーション機能を有する次世代ライブラリである\cite{detectron2}．
特徴としては，モデルの訓練速度及び推論速度が非常に高速であること，物体の検出数が多いことが挙げられる．
また，一部のプログラム書き換えと使用する学習済みデータの切り替えを行うことで，Classification（画像分類），Object Detection（物体認識）等の豊富な機能を簡単に実装することが可能となっている．

本研究では，動画内に映り込んでいる物体の物体名や位置情報などをデータベースに保存するために，Detectron2のObject Detection機能を使用する．
また，Detectron2で提供されている大規模な学習済みモデルも使用する．
データセットはCOCOモデルで学習したObject Detection向けに配布されているものである．
COCO（Microsoft COCO：Common Objects in Context）とは，画像内の物体の検出やセグメンテーションを行うための大規模なデータセットである\cite{coco}．

使用するデータセットの選択には，物体の検出が多いこと，そして検出時間が短いことが重要である．
Detectron2が公開しているデータセットは表\ref{tab:dataset}の通りである\cite{wu2019detectron2}．

\begin{table}[H]
  \centering
  \caption{Detectron2が公開しているデータセット}
  \label{tab:dataset}
  \begin{tabular}{c|cccccc}
    \toprule
    \thead{Name} & \thead{lr sched} & \thead{train time(s/iter)} & \thead{inference time(s/im)} & \thead{trainmem(GB)} & \thead{boxAP} \\
    \midrule
    R50-C4 & 1x & 0.551 & 0.102 & 4.8 & 35.7 \\
    R50-DC5 & 1x & 0.380 & 0.068 & 5.0 & 37.3 \\
    R50-FPN & 1x & 0.210 & 0.038 & 3.0 & 37.9 \\
    R50-C4 & 3x & 0.543 & 0.104 & 4.8 & 38.4 \\
    R50-DC5 & 3x & 0.378 & 0.070 & 5.0 & 39.0 \\
    R50-FPN & 3x & 0.209 & 0.038 & 3.0 & 40.2 \\
    R101-C4 & 3x & 0.619 & 0.139 & 5.9 & 41.1 \\
    R101-DC5 & 3x & 0.452 & 0.086 & 6.1 & 40.6 \\
    R101-FPN & 3x & 0.286 & 0.051 & 4.1 & 42.0 \\
    \bottomrule
  \end{tabular}
\end{table}

boxAPとは，物体検出の精度を示す指標である．
特に，AP（Average Precision）は，平均適合率と呼ばれ，全体の正解ラベルのうち，どれだけの割合が正しく検出できたかを平均的に表す指標のことである．
本システムでは，推論時間が短く，精度が高いR50-FPNを使用する．

\section{システム全体の構築と動作について}\label{chap3-3}
本システムの処理の流れを図\ref{fig:flow}に示す．

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/flow.png}
  \caption{システム全体の流れ}
  \label{fig:flow}
\end{figure}

Making Indexでは，動画のインデックス作成およびデータベースへの保存を行う．
ユーザーがアップロードした動画に対し，Detectron2を用いて動画の各フレームに対して解析を行い，その結果をデータベースへ保存する．
この処理を，画像を用いた検索の前にあらかじめ行うことで，画像を用いた検索結果がデータベースを介して高速に行えるようになる．

Search Videosでは，検索として入力した画像と一致や類似する動画のタイムスタンプを，データベースから検索する機能である．
ユーザーがアップロードした画像に対して，動画解析時と同様にDetectron2を用いた解析を行う．

本システムの解析にはDetectron2を用いるが，解析に使用するモデルを全て統一することで，同じ画像を複数回入力しても出力が全て一致する性質を利用する．

システムの全体には，Python製WebフレームワークFlaskを使用する．
画像解析時のPythonコードとシームレスな動作が可能な点，GUIライブラリを使用しなくても良い点などがFlaskを採用した理由である．

\subsection{データベースの作成について}\label{chap3-3-1}
本システムでは，ユーザー情報並びに動画，画像などの情報を，データベースを用いて管理している．
構築したデータベースは図\ref{fig:table_list}のとおりである．

\begin{figure}[t]
  \centering
  \fbox{\includegraphics[width=13cm]{image/table_list.png}}
  \caption{データベース図}
  \label{fig:table_list}
\end{figure}

画像検索を利用するには，動画の検索対象となるデータベースを作成するために，動画解析を行う必要がある．
ユーザーがシステムに動画をアップロードすると，自動的に動画解析が開始されるよう実装した．

動画解析では，アップロードされた動画の全フレームにCNNを用いた推論を実施する．
CNNはDetectron2の学習済みデータを用いることで一般的な物体に対して推論が可能となるため，その推論データをデータベースに格納する．
保存する情報は表\ref{tab:table_list}の通りである．

\begin{table}[t]
  \centering
  \caption{DetectedListsテーブルの詳細}
  \label{tab:table_list}
  \begin{tabular}{c|l}
    \toprule
    \thead{detected lists} & \thead{説明} \\ 
    \midrule
    number & 固有のID \\
    id & ユーザーID \\
    fps number & 動画のフレーム番号 \\
    detected name & 検出されたオブジェクトの物体名 \\
    detected score & 検出されたオブジェクトの認識率 \\
    detected x-min & 検出されたオブジェクトのx座標の最小値 \\
    detected y-min & 検出されたオブジェクトのy座標の最小値 \\
    detected x-max & 検出されたオブジェクトのx座標の最大値 \\
    detected y-max & 検出されたオブジェクトのy座標の最大値 \\
    center-x & 検出したオブジェクトの中心のx座標\\
    center-y & 検出したオブジェクトの中心のy座標 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{データベースの利用について}\label{chap3-3-2}
ユーザーがアップロードした動画の中で，自動で解析が完了したものについてはデータベースを用いた検索機能が利用できる．

ユーザーが画像をシステムにアップロードすると，動画のアップロード時と同様に自動で解析される．
解析では，画像に対してCNN（Detectron2）を用いた推論を行い，得られた情報をデータベースに保存する．
そして事前にデータベースに保存しておいた動画の解析データと画像の解析データを用いた検索を開始する．
検索方法は，
\begin{itemize}
  \item オブジェクトの位置情報を用いた検索
  \item オブジェクトの位置関係を用いた検索
\end{itemize}

の2種類を開発した．

\section{オブジェクトの位置情報を用いた検索}\label{chap3-4}
動画の解析データと画像の解析データを比較してオブジェクトの位置情報を用いた類似シーンを検索する手法について説明する．

処理の手順は下記のようになる．
\begin{enumerate}
  \item 入力した画像のオブジェクト検出リストをデータベースから取得
  \item 検索対象となっている動画の各フレームで検出したオブジェクト検出リストをデータベースから取得
  \item 1.と2.のオブジェクト検出リストを比較し，類似度を計算
\end{enumerate}

類似度を計算する際には，オブジェクトの位置情報を用いる．
その際，位置情報の比較には，動画と画像の$x,y$座標の実際の値ではなく，正規化した値を用いる．
仮に，$x,y$座標で実際の値を使用した例を図\ref{fig:compare}に記載する．

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/compare.jpg}
  \caption{サイズの異なる画像の比較図}
  \label{fig:compare}
\end{figure}

図\ref{fig:compare}のように，データベース上の座標が画像サイズによって異なっていた場合は，単純に比較が難しい．
そのため，正規化を行うことでサイズの異なる画像も比較することが可能となる．
正規化は，画像のサイズを$w,h$とした時に，（\ref{eq:normalize}）式により行う．

\begin{equation}
  \label{eq:normalize}
  (x_{norm} = \frac{x}{w}, y_{norm} = \frac{y}{h})
\end{equation}

また，Detectron2の解析結果において，オブジェクトの検出順が一定でないことがある．
リストの比較を行う際に，検出順が一定でないと比較が難しい．
そのため，画像と動画のオブジェクト検出リストを比較する際には，検出順をソートしてから比較を行う．
ソートの方法は，オブジェクト名でソートし，さらにその中でオブジェクトの座標の$x$座標の昇順，$y$座標の昇順とする．

類似シーンとして出力するのは，類似度が一定以上の動画のフレームである．
今回，類似度のしきい値は，$x_{min}$，$x_{max}$，$y_{min}$，$y_{max}$のそれぞれの値が，
画像と動画のシーンを比較した際に，平均で0.1を下回る場合に類似シーンとして出力する．
また，スコアはオブジェクトの座標が完全に一致している場合を100\%となるように計算している．

% 出力結果
これらの処理を経て，ユーザーは図\ref{fig:result0_1}のように類似シーンの検索結果を得られる．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result0_1.png}
  \caption{位置情報の検索での結果表示}
  \label{fig:result0_1}
\end{figure}


\section{オブジェクトの位置関係を用いた検索}\label{chap3-5}
\label{sec:search_relationship}
動画の解析データと画像の解析データのオブジェクト間の位置関係を用いた類似シーンの検索について述べる．
類似シーンの検索には，下記のような場合に使用することができる．

\begin{itemize}
  \item 画像（動画のフレーム）中に映っているオブジェクトの位置関係が似ている場合
  \item 位置情報を用いた検索では，検索結果が少ない（検索結果がない）場合
  \item 画像（動画のフレーム）において，拡大や縮小，回転したシーンが含まれる場合
\end{itemize}


位置関係が類似しているデータは，各オブジェクト間の距離と角度を用いることで検出できると考えられる．
具体的な例を，図\ref{fig:object_relationship}に示す．

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/object_relationship.png}
  \caption{検索結果}
  \label{fig:object_relationship}
\end{figure}

図\ref{fig:object_relationship}からわかるように，検出したオブジェクトの位置情報をもとに，各オブジェクトの距離と角度を使用したデータを作成する．
距離と角度を用いることで，オブジェクト間の位置関係を表現することができる．

次に，距離と角度の計算方法を述べる．
この処理が実行されるのは，画像と動画のオブジェクト検出において，物体数および物体名が一致した場合に限られる．
物体数と物体名が一致する場合は，(\ref{eq:object_list})式に示すように，物体名と座標のリストが作成される．

\begin{eqnarray}
  \label{eq:object_list}
  [[\text{”オブジェクト名”},x_{min},y_{min},x_{max},y_{max},\text{UUID}],\cdots]
\end{eqnarray}

このリストでは，オブジェクト名の他に，$x$座標と$y$座標の最小値と最大値が含まれている．
なお，オブジェクト名が一致しているものが複数ある場合，オブジェクト名を基準としてソートなどの操作を行なった場合に，リスト内でのオブジェクトの識別が難しくなる．
リスト内での識別を容易にするために，オブジェクト名の代わりに固有のID（UUID）も付与する．
UUID (Universally Unique Identifier) とは，一意な識別子のことである．UUIDは128ビットの数値から構成され，重複する可能性が非常に低いことが知られている．
$x$と$y$の値は，直交座標系の値となっているため，距離と角度を計算するために，極座標系に変換しながらリストを作成する．

まず，全てのオブジェクトを最短経路で結ぶ図形を作成する．
角度の計算が容易になるよう，(\ref{eq:object_list})式のリストは$x$座標，$y$座標の昇順でソートする．
次に，(\ref{eq:object_list})式の先頭にあるオブジェクトを，最初のオブジェクトとし，水平方向からの角度を基準に，一番角度が小さいオブジェクトを次のオブジェクトとする．
この時，角度を求める際の点の座標は，それぞれのオブジェクトの中心点とする．
中心点は，(\ref{eq:center})式に示すように，$x$座標は$x_{min}$座標と$x_{max}$座標の平均値，$y$座標は$y_{min}$座標と$y_{max}$座標の平均値を用いる．

\begin{equation}
  \label{eq:center}
  \begin{split}
  x_{center} &= \frac{x_{min}+x_{max}}{2} \\
  y_{center} &= \frac{y_{min}+y_{max}}{2}
  \end{split}
\end{equation}

また，角度の計算には，(\ref{eq:angle})式を用いる．

\begin{eqnarray}
  \label{eq:angle}
  \theta = \tan^{-1} \frac{y_{center}}{x_{center}}
\end{eqnarray}

これらの処理を，(\ref{eq:object_list})式のリストの先頭から順に行い，全てのオブジェクトが選択されるまで繰り返す．

繰り返す処理が完了すると，全てのオブジェクトを最短経路で結ぶ図形を図\ref{fig:figure_2}のように作成することができる．

\begin{figure}[b]
  \begin{center}
    \includegraphics[width=15cm]{./image/figure_2.png}
    \caption{オブジェクトの最短経路で結ぶ図形}
    \label{fig:figure_2}
  \end{center}
\end{figure}

また，(\ref{eq:object_list})式のリストの順番は，図形の最短経路で結んだ順番になっている．

次に，オブジェクト間の距離と図形の内角を計算する．
(\ref{eq:object_list})式のリストの順番が図形の最短経路で結ぶ順番でソートされているため，図形の内角は次のように求めることができる．
まず，(\ref{eq:object_list})式から隣り合っている3点を選択し，2つの辺を作成する．
次に，その2つの辺のなす角度を計算することで，図形の内角を求めることができる．
2つの辺は，3つの点をそれぞれ($x_1$, $y_1$), ($x_2$, $y_2$), ($x_3$, $y_3$)とすると，(\ref{eq:edge})式で求める．

\begin{equation}
  \label{eq:edge}
  \begin{split}
    \vec{e_1} &= \left( \begin{array}{c} x_2-x_1 \\ y_2-y_1 \end{array} \right) \\
    \vec{e_2} &= \left( \begin{array}{c} x_3-x_1 \\ y_3-y_1 \end{array} \right)
  \end{split}
\end{equation}

角度の計算には，(\ref{eq:angle2})式を用いる．
\begin{eqnarray}
  \label{eq:angle2}
  \theta = \cos^{-1} \left( \frac{\vec{e_1} \cdot \vec{e_2}}{|\vec{e_1}| |\vec{e_2}|} \right)
\end{eqnarray}

この処理を(\ref{eq:object_list})式のリストの要素全てに対して行うことで，図形の全ての内角を計算することができる．

オブジェクト間の長さについては，(\ref{eq:edge})式で求めた$\vec{e_1}$で求めることができる．


以上の処理を入力された画像および動画の各フレームに対して行うことで，類似シーン検索の準備を行うことができる．


次に，最短経路で結ぶ順番でソートされたオブジェクトのリストを用いて，類似シーンの検索を行う．

入力された画像と動画のフレームが類似しているかを判定する基準として，以下の4つを用いる．
\begin{itemize}
  \item オブジェクトの検出数が同じであること
  \item オブジェクトの物体名が同じであること
  \item オブジェクト同士を最短経路で結ぶ図形のそれぞれに対応する辺の比率が同じであること
  \item オブジェクト同士を最短経路で結ぶ図形のそれぞれに対応する内角が同じであること
\end{itemize}

ただし，上記の条件では，オブジェクトの位置関係が100\%一致している必要があるため，類似シーンの検索には基準が厳しいと考えられる．

この類似シーンの検索の結果には，拡大や縮小，回転などの変形が含まれている場合にも，類似性が高いと評価したい．
そのため，下記の条件を追加する．
\begin{itemize}
  \item オブジェクト同士を最短経路で結ぶ図形のそれぞれに対応する辺の比率が似ていること
  \item オブジェクト同士を最短経路で結ぶ図形のそれぞれに対応する内角が似ていること
\end{itemize}
ここでの具体的な数値については設定せず，それぞれの結果をソートして，上位の結果を出力することで，類似シーンの検索を行う．

図形が少しでも変形すると，辺の比率や内角の大きさが変化するため，図形全体でどの程度ずれているかを最後に計算する．
計算する値は，図形の回転角度，オブジェクト間の長さの比率の平均，オブジェクト間の長さの比率の分散，オブジェクト間の角度の分散である．

オブジェクト間の辺の長さについては，辺の比率の平均を計算する．
この値は，図形の拡大・縮小の度合いを表す．
値が大きくなればなるほど，図形が拡大されていることを示し，値が小さくなればなるほど，図形が縮小されていることを示す．

また，長さの比率の分散を計算する．
オブジェクト間の角度については，角度の分散を計算する．
どちらの値も，この値が大きくなればなるほど，図形が変形されていることを示す．

また，回転した図形についても，類似性が高いと評価したいため，図形の回転角度も考慮する．
なお，回転角度の計算には，オブジェクト同士を最短経路で結ぶ図形のそれぞれに対応する辺同士の角度の平均を用いる．

以上の過程を経て，類似シーンの検索を行う．
結果の出力では，図形の回転角度，オブジェクト間の長さの比率の平均，オブジェクト間の長さの比率の分散，オブジェクト間の角度の分散を出力する．

図形の形が似ている場合の出力結果を最優先するため，図形のオブジェクト間の角度の分散を基準の出力のソートを行い，図\ref{fig:result0_2}のように結果を出力する．

% 出力結果の例はどんなの？
\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result0_2.png}
  \caption{位置関係の検索での結果表示}
  \label{fig:result0_2}
\end{figure}


\section{まとめ}\label{chap:3-6}
本システムの動画検索システムの基本的な動作について述べた．

\clearpage

\chapter{動作検証と考察}
\label{sec:consideration}

\section{はじめに}\label{chap4-1}
本章では，本システムの動作検証と考察を行う．

\section{位置情報を用いた検索システムについて}\label{chap4-2}
本システムの実際の検索結果をもとに，位置情報を用いた検索システムの精度を考える．
なお，動画については，フレーム全てを示すことが難しいため，動画の一定時間ごとに抽出したものを示すこととする．
検証には，検索対象となる動画を5つ用意し，全てアップロード，並びに解析済みである．
動画1つ目は，親子が向かい合っている動画である(図\ref{fig:movie1})．
図\ref{fig:movie2}は動画2つ目で，飛行機がプッシュバックしているシーンを撮影した動画である．
3つ目の動画として，時計のタイムラプス動画を用意した(図\ref{fig:movie3})．
4つ目の動画として，同じシーンが繰り返し出現する動画を用意した（図\ref{fig:movie4}）．
また，オブジェクトが大量に映っている動画も解析を行い，図\ref{fig:movie5}のような5つ目の動画を用意した．

\begin{figure}[H]
  \centering
  \includegraphics[width=13cm]{image/1.png}
  \caption{親子が向かい合っている動画}
  \label{fig:movie1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=13cm]{image/2.png}
  \caption{飛行機がプッシュバックしているシーンを撮影した動画}
  \label{fig:movie2}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=13cm]{image/3.png}
  \caption{時計しか映っていないタイムラプス動画}
  \label{fig:movie3}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=13cm]{image/4.png}
  \caption{同一シーンが繰り返される動画}
  \label{fig:movie4}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/5.png}
  \caption{オブジェクトが大量に映っている動画}
  \label{fig:movie5}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=13cm]{image/result_1_1.jpg}
  \caption{親子が向かい合っている動画のあるシーン}
  \label{fig:img_1_1}
\end{figure}
次に入力した画像と結果を示す．図\ref{fig:img_1_1}に示す．

入力した画像と検索結果の動画フレームがほぼ一致していることがわかる．
オブジェクトは，personとskateboardが検出されている．
スコアが一番高いものがベストマッチとして表示されているが，類似シーンがあれば，表\ref{tab:tab_1_1}のように類似度順で出力される．

\begin{table}[t]
  \centering
  \caption{図\ref{fig:img_1_1}の検索結果}
  \label{tab:tab_1_1}
  \begin{tabular}{ccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{score}  \\
    \midrule
    production ID\_4265040.mp4 & 0 & 98.48 \\
    production ID\_4265040.mp4 & 1 & 98.37 \\
    production ID\_4265040.mp4 & 13 & 96.42 \\
    production ID\_4265040.mp4 & 15 & 96.13 \\
    production ID\_4265040.mp4 & 14 & 96.12 \\
    production ID\_4265040.mp4 & 16 & 95.98 \\
    production ID\_4265040.mp4 & 17 & 95.69 \\
    production ID\_4265040.mp4 & 18 & 95.3 \\
    production ID\_4265040.mp4 & 19 & 95.11 \\
    production ID\_4265040.mp4 & 20 & 94.88 \\
    \bottomrule
  \end{tabular}
\end{table}

このように，オブジェクトの位置が完全に一致していない場合でも，入力した画像と類似しているフレームがある場合は検索結果に表示される．
なお，出力されたフレーム数に出力すると，$0,1,13,15,..$とほぼフレーム順で類似シーンが出力されていることがわかる．
production ID\_4265040.mp4の動画データの詳細を見てみると表\ref{tab:tab_1_1_1}のようになっており，フレームレートが29.97であるため，
動画の秒数に換算しても，0秒〜1秒の間で類似シーンが集中していることがわかる．
\begin{table}[t]
  \centering
  \caption{production ID\_4265040.mp4の動画データの詳細}
  \label{tab:tab_1_1_1}
  \begin{tabular}{c|c}
    \toprule
    \thead{項目} & \thead{数値} \\
    \midrule
    横の大きさ & 3840 \\
    縦の大きさ & 2160 \\
    フレームレート & 29.97 \\
    総フレーム数 & 510 \\
    長さ & 00:17 \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_1_2.jpg}
  \caption{飛行機を映した画像}
  \label{fig:img_1_2}
\end{figure}

次に入力した画像と結果を図\ref{fig:img_1_2}に示す．
こちらの画像も，検索したい動画のフレームを完全一致のフレームとして検索できていることがわかる．
オブジェクトは，airplaneとtruck，personが2つ検出されている．
画像とフレームのどちらも検出された座標が全く同じであるため，スコアが100として出力された．
上位の検索結果を表\ref{tab:tab_1_2}に示す．

\begin{table}[t]
  \centering
  \caption{図\ref{fig:img_1_2}の検索結果}
  \label{tab:tab_1_2}
  \begin{tabular}{ccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{score}  \\
    \midrule
    2022-12-31.mp4 & 467 & 100.0 \\
    2022-12-31.mp4 & 466 & 99.39 \\
    2022-12-31.mp4 & 465 & 98.97 \\
    2022-12-31.mp4 & 464 & 98.81 \\
    2022-12-31.mp4 & 468 & 98.52 \\
    2022-12-31.mp4 & 463 & 98.25 \\
    2022-12-31.mp4 & 469 & 97.83 \\
    2022-12-31.mp4 & 459 & 97.54 \\
    2022-12-31.mp4 & 470 & 97.31 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_1_3.jpg}
  \caption{時計の画像}
  \label{fig:img_1_3}
\end{figure}

次に，3枚目の入力画像と結果を図\ref{fig:img_1_3}に示す．
図\ref{fig:img_1_3}の検索結果では，スコアが99.67とかなり高いスコアで検出されている．
しかし，2つの画像を比べてみると，検出しているフレームが異なっていることがわかる．

上位の検索結果を表\ref{tab:tab_1_3}に示す．
\begin{table}[b]
  \centering
  \caption{図\ref{fig:img_1_3}の検索結果}
  \label{tab:tab_1_3}
  \begin{tabular}{ccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{score}  \\
    \midrule
    clock.mp4 & 283 & 99.67 \\
    clock.mp4 & 58 & 99.22 \\
    clock.mp4 & 8 & 99.21 \\
    clock.mp4 & 56 & 99.17 \\
    clock.mp4 & 187 & 99.16 \\
    clock.mp4 & 189 & 99.09 \\
    clock.mp4 & 52 & 99.04 \\
    clock.mp4 & 234 & 99.0 \\
    \bottomrule
  \end{tabular}
\end{table}

これまでの表\ref{tab:tab_1_1}，表\ref{tab:tab_1_2}の出力では検出されたフレーム番号が近いものが多かったが，表\ref{tab:tab_1_3}の出力では検出されたフレーム番号が離れているものが見受けられた．

次に，4枚目の入力画像と結果を図\ref{fig:img_1_4}に示す．
\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result_1_4.jpg}
  \caption{ディジタルヒューマンのイメージ画像}
  \label{fig:img_1_4}
\end{figure}

この画像は，図\ref{fig:movie4}の動画内で繰り返し出現するシーンの画像である．
図\ref{fig:img_1_4}の検索結果を表\ref{tab:tab_1_4}に示す．
\begin{table}[b]
  \centering
  \caption{図\ref{fig:img_1_4}の検索結果}
  \label{tab:tab_1_4}
  \begin{tabular}{ccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{score}  \\
    \midrule
    ロボット-人間-オフィス-将来-88219.mp4 & 63 & 98.01 \\
    ロボット-人間-オフィス-将来-88219.mp4 & 149 & 97.51 \\
    ロボット-人間-オフィス-将来-88219.mp4 & 234 & 96.47 \\
    ロボット-人間-オフィス-将来-88219.mp4 & 62 & 96.31 \\
    ロボット-人間-オフィス-将来-88219.mp4 & 578 & 95.62 \\
    ロボット-人間-オフィス-将来-88219.mp4 & 496 & 88.17 \\
    \bottomrule
  \end{tabular}
\end{table}

表\ref{tab:tab_1_4}の出力では，検出されたフレーム番号が飛び飛びになっている．
これは，図\ref{fig:img_1_4}の画像が，図\ref{fig:movie4}の動画内で繰り返し出現するシーンの画像であるためであると考えられる．
フレーム番号63と，フレーム番号149の結果を図\ref{fig:img_1_4_1}，図\ref{fig:img_1_4_2}に示す．
\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result_1_4_1.jpg}
  \caption{フレーム番号63の検出結果}
  \label{fig:img_1_4_1}
\end{figure}

\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result_1_4_2.jpg}
  \caption{フレーム番号149の検出結果}
  \label{fig:img_1_4_2}
\end{figure}

どちらも同じようなシーンであるが，フレーム位置が異なっているためそれぞれのフレームの結果が表\ref{tab:tab_1_4}で出力されている．


最後に，オブジェクトが大量に写っている画像として5枚目の入力画像と結果を図\ref{fig:img_1_5}に示す．

図\ref{fig:img_1_5}では，スコアが98.54とほぼ一致したシーンの検出ができている．
これ以外に検出された類似シーンはなかった．
これまでと比べ，類似シーンの検索出力が少ないことがわかる．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_1_5.jpg}
  \caption{カルーセルの様子}
  \label{fig:img_1_5}
\end{figure}

他のシーンの画像でもシーン検索が行われるか確認したい．
その図を図\ref{fig:img_1_5_1}に示す．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_1_5_1.jpg}
  \caption{図\ref{fig:img_1_5}とは別のシーンの様子}
  \label{fig:img_1_5_1}
\end{figure}

図\ref{fig:img_1_5_1}では，検索結果の出力が得られなかった．

\subsection{考察}\label{chap4-2-1}
入力した画像と近いシーンを検索することができた．
ほとんどの画像に対してスコアが95以上の高精度で出力されたことがわかる．

図\ref{fig:img_1_4}のような，同じシーンが繰り返し存在する動画に対しても，適切な検索がなされていることがわかった．

しかし，いくつか問題も見受けられた．
まず，図\ref{fig:img_1_3}の結果である．
検出精度がかなり高い割には，検出されたシーン（フレーム）にばらつきが見られる．
入力された画像と針の位置が類似していないように見える．

これは，この検索対象となった図\ref{fig:movie3}の動画は，時計自体の位置は固定されており，針の位置だけが変化している動画だからこそ起こる現象であると考えられる．
本システムの検索システムの特徴として，オブジェクトとして認識されるのは，事前に学習させたクラスのみとなっている．
今回使用している学習済みモデルは，COCOデータセットで学習させたモデルであるため，時計の針に関するクラスは存在せず，時計自体のクラスのみが検出された．
そのため，全てのフレームがほぼ一致しているシーンとみなされてしまっている．

本システムでは，検索対象がオブジェクト名と検出された座標であるため，その2つを満たしているフレームであれば，一致シーンとして出力される．
オブジェクトの色が異なったり，オブジェクトの向きが異なったりしても，検出された座標が同じであれば，一致シーンとして出力されてしまうことがわかった．

次に図\ref{fig:img_1_5}の結果である．
図\ref{fig:img_1_5}のようなオブジェクトの数が多い画像に対して，検索性能が低下してしまうことがわかった．
これは，動画解析ならびに検索システムの方法自体に問題があると考えられる．

動画解析にはDetectron2を用いているが，オブジェクトの数が多いと，1フレームに検出されるオブジェクトが多くなることは想定される．
そのオブジェクトの認識において，大量に映り込んでいるオブジェクトのうち，一部のオブジェクトの検出が難しいことが考えられる．
ある程度の精度で認識されたオブジェクトのみ検索対象としているため，認識率の低いオブジェクトに関しては，フレームが変化する毎に結果が変化してしまう．
同一（類似）シーンとして出力されるのは，入力された画像と動画の各フレームのオブジェクト検出数が一致していることが最低条件となっているため，フレーム毎に検出されるオブジェクトの数が安定しないとなると，一致するシーンが出力されないことが考えられる．

\section{位置関係を用いた検索システムについて}\label{chap4-3}
位置情報を用いた検索システムの精度についても考える．

\ref{chap4-2}節で用いた4つの動画に加え，類似するシーンが存在している動画2つも検索対象として用いる（図\ref{fig:movie7}，図\ref{fig:movie8}）．
\begin{figure}[b]
  \centering
  \includegraphics[width=11cm]{image/6.png}
  \caption{人と犬が映っている動画　その1}
  \label{fig:movie7}
\end{figure}

\begin{figure}[b]
  \centering
  \includegraphics[width=11cm]{image/7.png}
  \caption{人と犬が映っている動画　その2}
  \label{fig:movie8}
\end{figure}

次に入力した画像と結果を示す．

まず，図\ref{fig:img_1_2}の例における，類似シーンの検索結果を表\ref{tab:tab_2_1}に示す．

\begin{table}[t]
  \centering
  \caption{図\ref{fig:img_1_2}の検索結果（【角度の分散】で昇順ソート）}
  \label{tab:tab_2_1}
  \begin{tabular}{cccccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{図形の傾き} & \thead{長さの平均} & \thead{長さの分散} & \thead{角度の分散} \\
    \midrule
    2022-12-31.mp4 & 467 & 0.0 & 1.0 & 0.0 & 0.0 \\
    2022-12-31.mp4 & 459 & 0.13 & 0.9953 & 0.0 & 0.0069 \\
    2022-12-31.mp4 & 464 & 0.09 & 0.9986 & 0.0 & 0.034 \\
    2022-12-31.mp4 & 463 & 0.22 & 0.9963 & 0.0002 & 0.0493 \\
    2022-12-31.mp4 & 453 & 0.06 & 1.2929 & 0.0002 & 0.0497 \\
    2022-12-31.mp4 & 466 & 0.1567 & 0.9997 & 0.0001 & 0.0557 \\
    2022-12-31.mp4 & 465 & 0.19 & 0.9995 & 0.0001 & 0.0718 \\
    2022-12-31.mp4 & 429 & 0.7167 & 0.9858 & 0.0001 & 0.0728 \\
    2022-12-31.mp4 & 257 & 0.2933 & 1.3556 & 0.0096 & 0.1384 \\
    2022-12-31.mp4 & 468 & 0.3267 & 1.0025 & 0.0005 & 0.1405 \\
    $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ \\
    2022-12-31.mp4 & 512 & 1.2467 & 1.0423 & 0.0164 & 4.9983 \\
    2022-12-31.mp4 & 68 & 6.83 & 1.2773 & 0.0121 & 8.5444 \\
    \bottomrule
  \end{tabular}
\end{table}


一番上に表示されている467フレームは，前のセクションの検証画像2と同じシーンで，検出されたオブジェクトの位置関係にズレがないことがわかる．
フレーム番号459のシーンを図\ref{fig:img_2_1_2}に示す．

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_1_2.jpg}
  \caption{図\ref{fig:img_1_2}の459フレームでの検出結果}
  \label{fig:img_2_1_2}
\end{figure}

図\ref{fig:img_2_1_2}の左側は入力した画像で，右側は検索結果のフレームである．
なお，それぞれの画像の中で，赤色と青色の図形が描画されている．
これは，検出されたオブジェクトの中心座標を結んでできる，オブジェクト間の位置関係を示したものである．
赤色は，入力画像内のオブジェクトの位置関係であり，青色は検索結果のフレーム内のオブジェクトの位置関係を示している．
この2つの図形はほぼ一致しており，位置関係が類似していることがわかる．
表\ref{tab:tab_2_1}のデータより，角度の分散が$0.0069$，長さの平均が$0.9953$，傾きも$0.13$と，入力画像の位置関係とほぼ一致していることがわかる．

また，フレーム番号453のシーンを図\ref{fig:img_2_1_3}に示す．

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_1_3.jpg}
  \caption{図\ref{fig:img_1_2}の453フレームでの検出結果}
  \label{fig:img_2_1_3}
\end{figure}

この図\ref{fig:img_2_1_3}の赤い線と青い線の関係が，相似であるかのように見える．
表\ref{tab:tab_2_1}のデータ中で，角度の分散が$0.0497$，図形の傾きが$0.06$と小さい値の中，長さの平均が$1.2929$となっている．
これより，図形の角度がほぼ一致しているが，図形自体の大きさが異なっていることがわかる．
実際，図\ref{fig:img_2_1_3}では，画像よりも少し画角の引かれたシーンが類似シーンとして出力されている．

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_1_4.jpg}
  \caption{図\ref{fig:img_1_2}の68フレームでの検出結果}
  \label{fig:img_2_1_4}
\end{figure}
また，参考として，傾きが一番大きいフレーム番号68のシーンを図\ref{fig:img_2_1_4}に示す．
図\ref{fig:img_2_1_4}の赤い線と青い線の関係は，傾きが$6.28$と検索結果の中で最大である．
長さの平均は$1.2675$，長さの分散は$0.0102$，角度の分散は$3.6179$となっている．
こういった動画の中で，図形の位置関係が類似している場合は，動画内で傾いたシーンであっても検索結果として出力できることがわかった．

このように，\ref{chap4-2}節では検索結果として出力されなかったシーンでも，こちらの検索では類似シーンとして出力されている．
\ref{chap4-2}節と比べて，より幅広い検索結果を得ることができる．

他の画像についても同様に検索を行なった．
入力した画像を図\ref{fig:img_2_2_1}に示し，検索結果上位項目を表\ref{tab:tab_2_2}に示す．

\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result2_2_2.jpg}
  \caption{入力画像}
  \label{fig:img_2_2_1}
\end{figure}

\begin{table}[b]
  \centering
  \caption{図\ref{fig:img_2_2_1}の検索結果（【図形の傾き】で昇順ソート）}
  \label{tab:tab_2_2}
  \begin{tabular}{cccccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{図形の傾き} & \thead{長さの平均} & \thead{長さの分散} & \thead{角度の分散} \\
    \midrule
    犬をカメラ$\cdots$.mp4 & 162 & 0.01 & 1.0059 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 197 & 0.04 & 0.9053 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 348 & 0.04 & 0.7887 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 349 & 0.04 & 0.7825 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 169 & 0.1 & 0.8994 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 181 & 0.1 & 0.8625 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 390 & 0.18 & 0.7998 & 0.0 & 0.0 \\
    犬を抱いて$\cdots$.mp4 & 292 & 0.18 & 0.9966 & 0.0 & 0.0 \\
    犬をカメラ$\cdots$.mp4 & 129 & 0.22 & 1.6378 & 0.0 & 0.0 \\
    \bottomrule
  \end{tabular}
\end{table}

表\ref{tab:tab_2_2}中の動画タイトルに注目すると，複数の動画から類似シーンを検出していることがわかる．
また，長さの分散と角度の分散がすべて0.0という出力になっていることもわかる．
検出されたオブジェクトが2つのため，図形内の角度が存在しない．そのため，図形の傾きと長さの平均のみの出力となっている．

図形の傾きの値が最も小さいフレーム番号162と，動画タイトルが他の結果と異なっているフレーム番号292の結果を図\ref{fig:img_2_2_3}，図\ref{fig:img_2_2_4}に示す．
\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result2_2_3.jpg}
  \caption{図形の傾きが小さいフレーム番号162}
  \label{fig:img_2_2_3}
\end{figure}

\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result2_2_4.jpg}
  \caption{動画タイトルの異なっているフレーム番号292}
  \label{fig:img_2_2_4}
\end{figure}

図\ref{fig:img_2_2_3}では，犬と女性がレンズに近づいているシーンが検出されている．
女性と犬の図形関係で，赤い線と青い線（入力画像の図形と検出された図形の関係）の角度がほとんど一致しており，入力した画像とほぼ同じ構図のシーンが検出されている．
図\ref{fig:img_2_2_3}の2枚の画像をよく比較してみると，女性の会と犬の重なりが少し違っていたり，犬の顔の位置が少し違っていたりする．
しかし，2つのオブジェクトの位置関係がほぼ同じであるため，類似シーンとして検出された．

図\ref{fig:img_2_2_4}では，入力された画像とほぼ一致しているシーンが検出されている．
こちらの画像は，入力画像との相違点が見られない．
オブジェクトの位置情報を用いた検索でも検出されそうなシーンでも，入力画像とフレームの類似度を計算して出力されている．

このように，解析したすべての動画の中から，幅広く類似シーンの検出を行なっていることがわかる．

\subsection{特殊な検索方法}
次に，本システムの位置関係を用いた検索でしか出来ない検索方法で類似シーンの検索を行う．

この検索を行うにあたり，図\ref{fig:movie6}も検索対象として用いた．
図\ref{fig:movie6}は，新しい解析対象の動画として加えた，少年がスポーツカーに手を振っている様子を撮影した動画である．
\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/8.png}
  \caption{少年がスポーツカーに手を振っている動画}
  \label{fig:movie6}
\end{figure}

図\ref{fig:img_2_3}に示すように，動画中から欲しいシーンの構図を手作りした画像で検索を行ってみる．
この画像は，オブジェクト間の位置関係がある程度一致していれば，厳密でなくても動画中から類似するシーンの検索が可能となる．
なお，図\ref{fig:img_2_3}では，少年が手を振っている前を車が通過するシーンを，動画中から検索するためにPowerPointで作成した画像である．
\begin{figure}[b]
  \centering
  \fbox{\includegraphics[width=13cm]{image/result_2_3.jpg}}
  \caption{入力画像}
  \label{fig:img_2_3}
\end{figure}

検索結果を表\ref{tab:tab_2_3}に示す．
\begin{table}[b]
  \centering
  \caption{図\ref{fig:img_2_3}の検索結果（【図形の傾き】で昇順ソート）}
  \label{tab:tab_2_3}
  \begin{tabular}{cccccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{図形の傾き} & \thead{長さの平均} & \thead{長さの分散} & \thead{角度の分散} \\
    \midrule
    道路沿い.mp4 & 419 & 0.07 & 4.1498 & 0.0 & 0.0 \\
    道路沿い.mp4 & 418 & 0.58 & 4.3203 & 0.0 & 0.0 \\
    道路沿い.mp4 & 417 & 3.94 & 4.7701 & 0.0 & 0.0 \\
    道路沿い.mp4 & 421 & 6.31 & 3.9212 & 0.0 & 0.0 \\
    道路沿い.mp4 & 422 & 6.65 & 3.7456 & 0.0 & 0.0 \\
    道路沿い.mp4 & 190 & 10.86 & 1.0588 & 0.0 & 0.0 \\
    道路沿い.mp4 & 191 & 11.11 & 1.1115 & 0.0 & 0.0 \\
    道路沿い.mp4 & 192 & 11.21& 1.1392 & 0.0 & 0.0 \\
    道路沿い.mp4 & 398 & 11.24 & 3.6778 & 0.0 & 0.0 \\
    道路沿い.mp4 & 195 & 11.48 & 1.2486 & 0.0 & 0.0 \\
    \bottomrule
  \end{tabular}
\end{table}

表\ref{tab:tab_2_3}に示すように，図形の傾きが一番近いものは，図\ref{fig:img_2_3_1}である．

\begin{figure}[b]
  \centering
  \includegraphics[width=13cm]{image/result_2_3_1.jpg}
  \caption{図\ref{fig:img_2_2_1}のベストマッチシーン（ソート：図形の傾き）}
  \label{fig:img_2_3_1}
\end{figure}

2つのオブジェクトを結ぶ線分の傾きがほぼ一致しており，類似シーンとして出力された．
オブジェクトの位置関係の傾きが一番近いものを抽出したい場合は，図\ref{fig:img_2_3}で傾きを意識して検索することで，類似シーンを得ることができる．

\begin{table}[b]
  \centering
  \caption{図\ref{fig:img_2_3}の検索結果（【長さの平均】で昇順ソート）}
  \label{tab:tab_2_4}
  \begin{tabular}{cccccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{図形の傾き} & \thead{長さの平均} & \thead{長さの分散} & \thead{角度の分散} \\
    \midrule
    道路沿い.mp4 & 189 & 14.19 & 1.006 & 0.0 & 0.0 \\
    道路沿い.mp4 & 190 & 10.86 & 1.0588 & 0.0 & 0.0 \\
    道路沿い.mp4 & 191 & 11.11 & 1.1115 & 0.0 & 0.0 \\
    道路沿い.mp4 & 192 & 11.21 & 1.1392 & 0.0 & 0.0 \\
    道路沿い.mp4 & 193 & 11.5 & 1.1931 & 0.0 & 0.0 \\
    道路沿い.mp4 & 195 & 11.48 & 1.2486 & 0.0 & 0.0 \\
    道路沿い.mp4 & 196 & 11.71 & 1.2752 & 0.0 & 0.0 \\
    道路沿い.mp4 & 197 & 12.05 & 1.3159 & 0.0 & 0.0 \\
    道路沿い.mp4 & 198 & 12.44 & 1.3857 & 0.0 & 0.0 \\
    道路沿い.mp4 & 199 & 12.68 & 1.4449 & 0.0 & 0.0 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_3_2.jpg}
  \caption{図\ref{fig:img_2_3_1}のベストマッチシーン（ソート：長さの平均）}
  \label{fig:img_2_3_2}
\end{figure}

また，表\ref{tab:tab_2_4}に示すように，長さの平均が一番近いものは，図\ref{fig:img_2_3_2}である．
2つのオブジェクトの長さの平均がほぼ一致しており，類似シーンとして出力された．
スポーツカーが人の前を通過しているシーンが出力されている．
なお，検索の目的であった，少年がスポーツカーに対して手を振っているシーンは，表\ref{tab:tab_2_4}内の，フレーム番号193あたりで見つけることができた（図\ref{fig:img_2_3_3}）．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_3_3.jpg}
  \caption{目的のシーン}
  \label{fig:img_2_3_3}
\end{figure}

次に，オブジェクトが3つ以上ある場合について考察を行う．
図\ref{fig:movie1}から，2人の位置が近い状態のシーン検索を行いたい．
図\ref{fig:img_2_3}で行なったように，PowerPointを使用してイメージ図を作成した（図\ref{fig:img_2_4}）．
この画像を入力として検索してみたところ，位置情報と位置関係のどちらの検索結果とも類似シーンが出力されなかった．
入力画像の解析結果は，図\ref{fig:img_2_4_1}に示す．
\begin{figure}[h]
  \centering
  \fbox{
    \includegraphics[width=105mm]{image/result_2_4.PNG}
  }
  \caption{PowerPointで作成した入力画像}
  \label{fig:img_2_4}
\end{figure}

\begin{figure}[h]
  \centering
  \fbox{
    \includegraphics[width=105mm]{image/result_2_4_1.PNG}
  }
  \caption{図\ref{fig:img_2_4}の解析結果}
  \label{fig:img_2_4_1}
\end{figure}

解析結果を見ると，入力した画像で，検出されているオブジェクトが2つ（personとteddy bear）であることがわかる．
右側のキックボードに乗っている少女が，teddy bear（熊のぬいぐるみ）として認識されていたため，目的の検索結果が得られなかったとが考えられる．
Detectron2の学習済みデータを使用して入力画像の解析を行っており，画像によっては正しく認識できないことがある．

\begin{table}[t]
  \centering
  \caption{図\ref{fig:movie1}の解析結果}
  \label{tab:tab_2_5}
  \begin{tabular}{cc}
    \toprule
    \thead{オブジェクト名} & \thead{認識率[\%]}  \\
    \midrule
    person & 99.94 \\
    person & 99.9 \\
    skateboard & 98.26 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \fbox{
    \includegraphics[width=14cm]{image/result_2_4_2_2.PNG}
  }
  \caption{PowerPointで作成した入力画像の解析結果}
  \label{fig:img_2_4_2}
\end{figure}

\begin{table}[t]
  \centering
  \caption{図\ref{fig:img_2_4_2}の検索結果（【図形の傾き】で昇順ソート）}
  \label{tab:tab_2_6}
  \begin{tabular}{cccccc}
    \toprule
    \thead{動画タイトル} & \thead{対象フレーム} & \thead{図形の傾き} & \thead{長さの平均} & \thead{長さの分散} & \thead{角度の分散} \\
    \midrule
    production$\cdots$.mp4 & 356 & 8.25 & 1.1921 & 0.0055 & 10.7487 \\
    production$\cdots$.mp4 & 357 & 8.58 & 1.2108 & 0.0071 & 10.8384 \\
    production$\cdots$.mp4 & 358 & 8.67 & 1.224 & 0.0114 & 13.5234 \\
    production$\cdots$.mp4 & 355 & 7.445 & 1.1973 & 0.0043 & 13.7131 \\
    production$\cdots$.mp4 & 352 & 6.385 & 1.1459 & 0.0022 & 15.7873 \\
    production$\cdots$.mp4 & 354 & 6.98 & 1.1819 & 0.0004 & 17.1475 \\
    production$\cdots$.mp4 & 359 & 8.895 & 1.2697 & 0.0164 & 17.6553 \\
    production$\cdots$.mp4 & 345 & 5.655 & 1.0659 & 0.0014 & 17.9837 \\
    production$\cdots$.mp4 & 353 & 6.73 & 1.1585 & 0.0034 & 18..0178 \\
    production$\cdots$.mp4 & 344 & 5.655 & 1.0544 & 0.0028 & 18.629 \\
    \bottomrule
  \end{tabular}
\end{table}

そこで，検索対象となるべき動画（図\ref{fig:movie1}）について，どのようなオブジェクトが検出されているかを確認を行なった結果を表\ref{tab:tab_2_5}に示す．
これより，動画内で検出されているオブジェクトは，personとskateboardであることがわかった．
そこで，personとskateboardを使用した画像を改めて作成し，入力として検索を行なった結果を図\ref{fig:img_2_4_2}，表\ref{tab:tab_2_6}に示す．
表\ref{tab:tab_2_6}のように，多くの類似シーンの検出ができた．
図形の傾きが一番小さい356フレーム目の検出結果を図\ref{fig:img_2_4_3}に示す．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_4_3.jpg}
  \caption{図\ref{fig:img_2_4}の検出結果（フレーム番号：356）}
  \label{fig:img_2_4_3}
\end{figure}

このように，動画内に目的のシーンのオブジェクト構成がわかる場合は，
そのシーンと類似している画像を自作することで，目的のシーンの検索が可能となっている．
しかし，必ずしも目的のシーンが検索できるわけではない．
検索する画像内のオブジェクトの認識が難しい場合や，Detectron2で識別できない場合などは，検索したい動画の解析結果を確認しながら画像の作成，並びに検索を行う必要がある．

\subsection{考察}\label{chap4-3-1}
本節の提案手法では，画像に一致している動画だけでなく，画像および動画内のオブジェクトの関係性を考慮して検索を行うことで，より幅広い検索が可能となった．
しかし，充分に使いやすい機能とは言えないと考える．
それは，検索結果の出力である．

図\ref{fig:img_2_3}での検索結果が非常にわかりづらい．
どの項目でソートすると最も類似したシーンになるのか，目的のシーンになるのかが不明瞭であるためである．
現在，ソート機能では，図形の傾きや角度の分散など，1項目に限って行える．
しかし，ある場面では角度の分散と長さの平均の2項目をソートに利用したい場合もある．
こういった場面に対応できていないため，検索結果の出力については改善の余地があると考える．

また，検出されたオブジェクト数が2つの場合は，3つ以上と比べて検索機能が不十分であると考える．
3つ以上のオブジェクト時には，必ずオブジェクト関係間で内角が存在するため，内角の分散を考慮することができる．
しかし，2つのオブジェクトの場合は，内角が存在しないため，内角の分散を考慮することができない．
また，辺も1つしかないため，辺の長さの分散も考慮することができない．
そのため，2つのオブジェクトの場合は，図形の傾きと長さの平均のみしか考慮できず，目的のシーンの検索が難しいと考える．
オブジェクトが1つの場合は，図形の傾き，長さの平均，長さの分散，角度の分散の全てを計算することができず，類似シーンの検索には，オブジェクト名の一致のみが考慮される．
これらの問題点を解決するために，オブジェクト数に応じて，検索機能を改善する必要があると考える．

オブジェクトが大量に画像内に存在する場合の，図形の計算についても考察する．
本システムの図形の計算では，\ref{sec:search_relationship}節で説明したとおりである．
図形のオブジェクトを描画するために，オブジェクトの点を全て通りながら外周を描くようなルートを計算している．
現在の計算において，オブジェクトの数が少ない場合は，図\ref{fig:img_2_4_4}のように，図形が正しく描画できる．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_4_4.png}
  \caption{オブジェクトが4つの図形の描画}
  \label{fig:img_2_4_4}
\end{figure}

しかし，オブジェクトが多数ある場合，例えば図\ref{fig:img_2_4_5}（オブジェクトの数が9つ）のように，図形が正しく描画されていない．
\begin{figure}[t]
  \centering
  \includegraphics[width=13cm]{image/result_2_4_5.png}
  \caption{オブジェクトが9つの図形の描画}
  \label{fig:img_2_4_5}
\end{figure}

図形が図\ref{fig:img_2_4_4}のように単純な図形であれば本システムの図形の計算でも問題ないが，
複雑な図形の場合は正しく描画することができないこともあるとわかった．

類似シーンの検索精度向上には，この図形の計算の改善が必要であると考える．
特に，グラフ理論などを用いることでより正しい計算ができるようになるのではないか．

\section{全体を通しての考察}\label{chap4-4}

繰り返し本システムを利用した結果，検索結果の表示までにかかる時間が長いことが分かった．

そこで，データベースと処理時間の関係について検証を行なった．
まず，動画解析にかかる時間を調べた．
その結果を図\ref{fig:img_2_5}に示す．
\begin{figure}[t]
  \centering
  \fbox{
    \includegraphics[width=13cm]{image/result_2_5.png}
  }
  \caption{DBサイズと検出時間の関係}
  \label{fig:img_2_5}
\end{figure}

図\ref{fig:img_2_5}の横軸はデータベースのサイズを，縦軸は動画解析にかかる時間である．
動画解析の結果をデータベースに登録する際には，データベースのサイズに関わらず，オブジェクトあたりの解析時間はほぼ一定であることがわかった．

次に，検索時間について調べた．
位置情報を使用した検索にかかる時間を図\ref{fig:img_2_5_2}に，位置関係を使用した検索にかかる時間を図\ref{fig:img_2_5_3}に示す．
検索には画像内のオブジェクト数が異なる2種類の画像を使用した．
1枚目の画像は，画像内に3つのオブジェクトが存在する．
2枚目の画像は，画像内に8つのオブジェクトが存在する．
これにより，オブジェクト数によって検索時間がどのように変化するかもを調べた．
\begin{figure}[t]
  \centering
  \fbox{
    \includegraphics[width=13cm]{image/result_2_5_2.png}
  }
  \caption{DBサイズと検索時間の関係(位置情報を使用)}
  \label{fig:img_2_5_2}
\end{figure}

\begin{figure}[t]
  \centering
  \fbox{
    \includegraphics[width=13cm]{image/result_2_5_3.png}
  }
  \caption{DBサイズと検索時間の関係(位置関係を使用)}
  \label{fig:img_2_5_3}
\end{figure}

図\ref{fig:img_2_5_2}，図\ref{fig:img_2_5_3}の横軸はデータベースのサイズ，縦軸はデータベースにアクセスする時間をアクセスした回数で割った『データ1つを取得するのにかかる時間(トランザクションタイム)』である．
トランザクションタイムの計算式は，以下の通りである．
\begin{equation}
  トランザクション時間[ms] = \frac{レスポンス時間[ms]}{データベースにアクセスした回数[times]}
\end{equation}

これらの図より，データベースのサイズが増加すると，データベースにアクセスする時間（トランザクションタイム）が増加していることがわかった．
特に，データベースのサイズが2,000KBまではトランザクションタイムが非常に短い．
その後は，サイズに比例してトランザクションタイムが急激に増加している．
また，図\ref{fig:img_2_5_2}，図\ref{fig:img_2_5_3}とも，画像内のオブジェクト数が少なくなるほどトランザクションタイムが短くなることがわかった．

以上のことから，動画解析にかかる時間はデータベースのサイズに関係なく一定であるが，動画検索にかかる時間はデータベースのサイズに比例して増加していることがわかった．
検索時間を減らすためには，データベースに保存するデータ数の削減や，データベースとやり取りを行う回数（トランザクション回数）の削減などを行う必要があると考える．

また，学習済みオブジェクト検出モデルを使用しているため，そのモデルで検出できないオブジェクトがあると，検索結果に表示されないという問題もある．
これは従来のRGB値やヒストグラムを使用した比較の時には生じない．
どのような映像であろうと，RGB値やヒストグラムを比較することで，同一シーンの瞬間を検索することができる．

しかし，今回使用している学習済みモデルは，\ref{sec:system}章でも述べたとおりCOCOデータセットを使用しているため，そのデータセットに含まれていないオブジェクトは検出できない．
オブジェクトが検出できないと，同一シーンだけでなく，類似シーンの検索結果も表示されない．

この問題を解決するには，目的にあった学習済みモデルを使用する必要がある．
例えば，ゲーム画面に特化した検索システムを構築する場合，ゲーム画面のデータセットを用いてモデルを学習させた方が検索システムの精度が向上すると考えられる．
また，将棋の駒を学習データとしてモデルを作成すると，将棋の特定の盤面を検索することも可能であると考える．

今回使用しているDetectron2には，カスタムデータセットから推論を行う機能があるため，アノテーションデータが用意できれば，オリジナルのモデルを作成することも可能である．

一般的な映像を検索するシステムの構築を目標とした本研究では，Detectron2の学習済みモデルを使用しているが，今後は目的に合わせた学習済みモデルを使用することで，より高い精度の検索システムを構築することができると考える．

\section{まとめ}\label{chap4-5}
位置情報を用いた検索システムと，位置関係を用いた検索システムのそれぞれの動作と結果について考察を行なった．


\chapter{結論}
\label{sec:conclusion}
% まだ修正できていない．
本研究では，画像を入力として，その画像に含まれるオブジェクトの位置情報や位置関係を用いることによる検索システムの構築を行なった．

第1章では，本研究の背景や目的について述べた．

第2章では，本研究で使用する画像処理の手法について述べた．
特に，ニューラルネットワーク（ANN）の基本事項や，CNNを用いた画像認識について述べた．

第3章では，本研究で使用する検索システムの構築について述べた．
画像解析にはDetectron2を使用し，Python製フレームワークFlaskを用いたシステムの構築について述べた．
また，Detectron2で検出されたオブジェクトの位置情報や位置関係を用いた検索システムの計算方法や，検索結果の表示方法について述べた．

第4章では，本研究の動作結果と考察について述べた．
位置情報を用いた検索を行うことで，一致シーンの検出を行うことが確認できた．
また，類似シーンについても検索結果に表示することができた．
しかし，オブジェクトの数が少ない場合や，オブジェクトの位置関係が複雑な場合には，検索結果が正しく表示されないことが確認された．
位置関係を用いた検索では，多くの類似シーンを検索結果に出力することができた．
また，動画の切り出し画像ではなく，外部ツールを用いて作成したイメージ画像でも目的にシーンの検索を行うことができた．
特に，位置関係を重要とする映像（例えば，スポーツ映像でダブルスの試合の映像や，複数人でのダンスレッスンの映像）に対して，より実用的な類似シーンを提示できると考えられる．

システム全体として，これまでフレーム毎のRGB値やヒストグラム比較で行われていた画像・動画検索を，CNNの解析結果とデータベースを元にした新しい検索システムの構築を行うことができた．
今後の展望として，より実用的なシステムの構築を行うべく，検索精度の向上や検索速度向上などを課題として引き継ぐ．

\chapter*{謝辞}
\addcontentsline{toc}{chapter}{謝辞}
本研究の実施の機会を与えてくださり，終始丁寧かつ的確な御指導，御鞭撻をいただきました准教授　藤原　和彦　氏に深謝の意を表し，厚く御礼申し上げます．\par
また，日常的に有益な議論をしていただいた研究室の皆様に感謝いたします．

\clearpage


\bibliographystyle{junsrt}
\urlstyle{same}
\bibliography{sotsuron2.bib}

\end{document}
